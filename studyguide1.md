# Topics
### Preliminary Answers Without Looking at Notes/Slides: 
    1. Given a problem, can you tell if it is supervise vs unsupervised learning or regression vs classification problem?
    Answer: supervised versus unsupervised has to do with the labels. With labels is supervised learning while without labels is unsupervised. Both require outputs (predictions). regression gives scalar output f(x) = wx + b while classification's output                   f(x) is a binary value {0,1}.
    2. Be comfortable with linear regression; how do you define it mathematically? What are the parameters? How do you find optimal parameters?
    3. Why do we need cost function? What is gradient decent and why it works? How we update a parameter in each iteration?
    4. What is the impact of learning rate on finding the optimal parameter?
    5. Global vs. local optimum?
    6. Why do we scale input data?
    7. What is feature engineering?
    8. Why is vectorization helpful?
    9. Be comfortable with logistic regression; how do you define it mathematically? What are the parameters? How do you find optimal parameters?
    10. How are decision boundaries defined in logistic regression? 
    11. What is overfitting?
    12. What is regularization and how is it helpful?
    13. How does regularization change the cost function and update formula in gradient descent?
    14. Why train/test split?
    15. Why train/test/validation split?
    16. What is the right approach in model selection?
    17. What is k-fold CV?
    18. What is bias vs. variance? What is overfitting vs. underfitting?
    19. What is a decision tree and how do you train it?
    20. How is information gain calculated for classification problems? ore regression problems?
    21. One-hot encoding?
    22. What is disadvantage of decision trees? How can this be improved?
    23. What is bagging?
    24. How does RF work? How does Gradient Boosting work?
    25. What is feature importance in tree ensembles?
